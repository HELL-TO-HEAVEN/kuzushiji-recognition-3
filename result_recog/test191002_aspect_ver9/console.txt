Using TensorFlow backend.
 read image 3881/3104
 read image 3864/777
WARNING: Logging before flag parsing goes to stderr.
W1005 10:39:34.575912  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W1005 10:39:34.903916  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W1005 10:39:34.997677  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

W1005 10:39:35.122616  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

W1005 10:39:35.122616  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2019-10-05 10:39:35.138200: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-10-05 10:39:35.153524: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-10-05 10:39:35.303845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
2019-10-05 10:39:35.308780: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-10-05 10:39:35.313995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-05 10:39:37.223441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-05 10:39:37.227001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-10-05 10:39:37.228722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-10-05 10:39:37.234690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6351 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
W1005 10:39:37.684538  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

W1005 10:39:39.918422  9436 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 64, 64, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 48)   480         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 48)   0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 16, 16, 96)   41568       activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 16, 16, 96)   384         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 16, 16, 96)   0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 16, 16, 96)   83040       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 16, 16, 96)   384         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 16, 16, 96)   0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 16, 16, 96)   83040       activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 16, 16, 96)   384         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 16, 16, 96)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 16, 16, 96)   83040       activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 16, 16, 96)   384         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 16, 16, 96)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 16, 16, 96)   83040       activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 16, 16, 96)   384         conv2d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 16, 16, 96)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 96)   83040       activation_6[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 96)   4704        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 96)   384         conv2d_7[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 16, 16, 96)   0           conv2d_8[0][0]
                                                                 batch_normalization_7[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 8, 8, 192)    166080      add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 8, 8, 192)    768         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 8, 8, 192)    0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 8, 8, 192)    331968      activation_7[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 8, 8, 192)    768         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 8, 8, 192)    0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 8, 8, 192)    331968      activation_8[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 8, 8, 192)    768         conv2d_11[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 8, 8, 192)    0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 8, 8, 192)    331968      activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 8, 8, 192)    768         conv2d_12[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 8, 8, 192)    0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 8, 8, 192)    331968      activation_10[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 8, 8, 192)    768         conv2d_13[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 8, 8, 192)    0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 8, 8, 192)    331968      activation_11[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 192)    768         conv2d_14[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 8, 8, 192)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 192)    331968      activation_12[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 8, 8, 192)    768         conv2d_15[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 192)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 8, 8, 192)    331968      activation_13[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 8, 8, 192)    18624       add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 8, 8, 192)    768         conv2d_16[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 8, 8, 192)    0           conv2d_17[0][0]
                                                                 batch_normalization_15[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 4, 4, 384)    663936      add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 4, 4, 384)    1536        conv2d_18[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 4, 4, 384)    0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 4, 4, 384)    1327488     activation_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 4, 4, 384)    1536        conv2d_19[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 4, 4, 384)    0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 4, 4, 384)    1327488     activation_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 4, 4, 384)    1536        conv2d_20[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 4, 4, 384)    0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 4, 4, 384)    1327488     activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 4, 4, 384)    1536        conv2d_21[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 4, 4, 384)    0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 4, 4, 384)    1327488     activation_17[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 4, 4, 384)    1536        conv2d_22[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 4, 4, 384)    0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 4, 4, 384)    1327488     activation_18[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 4, 4, 384)    74112       add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 4, 4, 384)    1536        conv2d_23[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 4, 4, 384)    0           conv2d_24[0][0]
                                                                 batch_normalization_21[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 4, 4, 384)    1536        add_3[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 4, 4, 384)    0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 1)            0
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 384)          0           activation_19[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 1)            4           input_2[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 385)          0           global_average_pooling2d_1[0][0]
                                                                 batch_normalization_23[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 4212)         1625832     concatenate_1[0][0]
==================================================================================================
Total params: 11,991,148
Trainable params: 11,981,450
Non-trainable params: 9,698
__________________________________________________________________________________________________
W1005 10:39:40.371431  9436 deprecation.py:323] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/200
1071/1070 [==============================] - 440s 411ms/step - loss: 2.3655 - acc: 0.6954 - val_loss: 1.4670 - val_acc: 0.7896
Epoch 2/200
1071/1070 [==============================] - 526s 491ms/step - loss: 1.1451 - acc: 0.8422 - val_loss: 1.0250 - val_acc: 0.8587
Epoch 3/200
1071/1070 [==============================] - 569s 532ms/step - loss: 0.9116 - acc: 0.8726 - val_loss: 0.8515 - val_acc: 0.8888
Epoch 4/200
1071/1070 [==============================] - 537s 502ms/step - loss: 0.8025 - acc: 0.8875 - val_loss: 0.8448 - val_acc: 0.8823
Epoch 5/200
1071/1070 [==============================] - 514s 480ms/step - loss: 0.7312 - acc: 0.8973 - val_loss: 0.7159 - val_acc: 0.9076
Epoch 6/200
1071/1070 [==============================] - 528s 493ms/step - loss: 0.6819 - acc: 0.9036 - val_loss: 0.7420 - val_acc: 0.8998
Epoch 7/200
1071/1070 [==============================] - 488s 455ms/step - loss: 0.6396 - acc: 0.9097 - val_loss: 0.6491 - val_acc: 0.9160
Epoch 8/200
1071/1070 [==============================] - 530s 494ms/step - loss: 0.6090 - acc: 0.9132 - val_loss: 0.6071 - val_acc: 0.9228
Epoch 9/200
1071/1070 [==============================] - 548s 512ms/step - loss: 0.5807 - acc: 0.9170 - val_loss: 0.6611 - val_acc: 0.9075
Epoch 10/200
1071/1070 [==============================] - 514s 480ms/step - loss: 0.5567 - acc: 0.9194 - val_loss: 0.5824 - val_acc: 0.9239
Epoch 11/200
1071/1070 [==============================] - 533s 498ms/step - loss: 0.5334 - acc: 0.9228 - val_loss: 0.5778 - val_acc: 0.9219
Epoch 12/200
1071/1070 [==============================] - 511s 477ms/step - loss: 0.5157 - acc: 0.9250 - val_loss: 0.5631 - val_acc: 0.9242
Epoch 13/200
1071/1070 [==============================] - 497s 464ms/step - loss: 0.5003 - acc: 0.9266 - val_loss: 0.5463 - val_acc: 0.9255
Epoch 14/200
1071/1070 [==============================] - 480s 449ms/step - loss: 0.4873 - acc: 0.9282 - val_loss: 0.5060 - val_acc: 0.9341
Epoch 15/200
1071/1070 [==============================] - 515s 481ms/step - loss: 0.4747 - acc: 0.9296 - val_loss: 0.5209 - val_acc: 0.9304
Epoch 16/200
1071/1070 [==============================] - 525s 491ms/step - loss: 0.4642 - acc: 0.9313 - val_loss: 0.5285 - val_acc: 0.9266
Epoch 17/200
1071/1070 [==============================] - 519s 485ms/step - loss: 0.4538 - acc: 0.9329 - val_loss: 0.4912 - val_acc: 0.9344
Epoch 18/200
1071/1070 [==============================] - 464s 433ms/step - loss: 0.4465 - acc: 0.9333 - val_loss: 0.5005 - val_acc: 0.9323
Epoch 19/200
1071/1070 [==============================] - 487s 455ms/step - loss: 0.4372 - acc: 0.9346 - val_loss: 0.4675 - val_acc: 0.9393
Epoch 20/200
1071/1070 [==============================] - 508s 474ms/step - loss: 0.4295 - acc: 0.9355 - val_loss: 0.5006 - val_acc: 0.9316
Epoch 21/200
1071/1070 [==============================] - 463s 433ms/step - loss: 0.4225 - acc: 0.9359 - val_loss: 0.5314 - val_acc: 0.9215
Epoch 22/200
1071/1070 [==============================] - 481s 449ms/step - loss: 0.4170 - acc: 0.9371 - val_loss: 0.4650 - val_acc: 0.9380
Epoch 23/200
1071/1070 [==============================] - 478s 446ms/step - loss: 0.4094 - acc: 0.9378 - val_loss: 0.4647 - val_acc: 0.9372
Epoch 24/200
1071/1070 [==============================] - 460s 429ms/step - loss: 0.4038 - acc: 0.9388 - val_loss: 0.5005 - val_acc: 0.9270
Epoch 25/200
1071/1070 [==============================] - 492s 460ms/step - loss: 0.4002 - acc: 0.9389 - val_loss: 0.4462 - val_acc: 0.9403
Epoch 26/200
1071/1070 [==============================] - 463s 432ms/step - loss: 0.3968 - acc: 0.9391 - val_loss: 0.4672 - val_acc: 0.9338
Epoch 27/200
1071/1070 [==============================] - 444s 414ms/step - loss: 0.3915 - acc: 0.9402 - val_loss: 0.4513 - val_acc: 0.9375
Epoch 28/200
1071/1070 [==============================] - 496s 463ms/step - loss: 0.3877 - acc: 0.9404 - val_loss: 0.4376 - val_acc: 0.9411
Epoch 29/200
1071/1070 [==============================] - 454s 424ms/step - loss: 0.3830 - acc: 0.9416 - val_loss: 0.4364 - val_acc: 0.9410
Epoch 30/200
1071/1070 [==============================] - 456s 425ms/step - loss: 0.3781 - acc: 0.9418 - val_loss: 0.4672 - val_acc: 0.9335
Epoch 31/200
1071/1070 [==============================] - 480s 448ms/step - loss: 0.3760 - acc: 0.9419 - val_loss: 0.4242 - val_acc: 0.9441
Epoch 32/200
1071/1070 [==============================] - 446s 416ms/step - loss: 0.3721 - acc: 0.9424 - val_loss: 0.4497 - val_acc: 0.9367
Epoch 33/200
1071/1070 [==============================] - 451s 421ms/step - loss: 0.3708 - acc: 0.9427 - val_loss: 0.4331 - val_acc: 0.9395
Epoch 34/200
1071/1070 [==============================] - 443s 414ms/step - loss: 0.3670 - acc: 0.9431 - val_loss: 0.4341 - val_acc: 0.9404
Epoch 35/200
1071/1070 [==============================] - 449s 419ms/step - loss: 0.3642 - acc: 0.9433 - val_loss: 0.4149 - val_acc: 0.9442
Epoch 36/200
1071/1070 [==============================] - 453s 423ms/step - loss: 0.3610 - acc: 0.9437 - val_loss: 0.4241 - val_acc: 0.9422
Epoch 37/200
1071/1070 [==============================] - 434s 405ms/step - loss: 0.3578 - acc: 0.9440 - val_loss: 0.4254 - val_acc: 0.9418
Epoch 38/200
1071/1070 [==============================] - 469s 438ms/step - loss: 0.3559 - acc: 0.9444 - val_loss: 0.4181 - val_acc: 0.9426
Epoch 39/200
1071/1070 [==============================] - 445s 416ms/step - loss: 0.3527 - acc: 0.9449 - val_loss: 0.4390 - val_acc: 0.9374
Epoch 40/200
1071/1070 [==============================] - 454s 424ms/step - loss: 0.3504 - acc: 0.9450 - val_loss: 0.4255 - val_acc: 0.9395
Epoch 41/200
1071/1070 [==============================] - 464s 433ms/step - loss: 0.3474 - acc: 0.9456 - val_loss: 0.4428 - val_acc: 0.9357
Epoch 42/200
1071/1070 [==============================] - 445s 416ms/step - loss: 0.3475 - acc: 0.9450 - val_loss: 0.4534 - val_acc: 0.9320
Epoch 43/200
1071/1070 [==============================] - 435s 406ms/step - loss: 0.3444 - acc: 0.9459 - val_loss: 0.4124 - val_acc: 0.9422
Epoch 44/200
1071/1070 [==============================] - 443s 414ms/step - loss: 0.3408 - acc: 0.9466 - val_loss: 0.4146 - val_acc: 0.9414
Epoch 45/200
1071/1070 [==============================] - 460s 430ms/step - loss: 0.3391 - acc: 0.9467 - val_loss: 0.4146 - val_acc: 0.9432
Epoch 46/200
1071/1070 [==============================] - 422s 394ms/step - loss: 0.3383 - acc: 0.9466 - val_loss: 0.4057 - val_acc: 0.9440
Epoch 47/200
1071/1070 [==============================] - 455s 425ms/step - loss: 0.3386 - acc: 0.9462 - val_loss: 0.4560 - val_acc: 0.9319
Epoch 48/200
1071/1070 [==============================] - 442s 413ms/step - loss: 0.3344 - acc: 0.9468 - val_loss: 0.4405 - val_acc: 0.9329
Epoch 49/200
1071/1070 [==============================] - 427s 399ms/step - loss: 0.3344 - acc: 0.9472 - val_loss: 0.4137 - val_acc: 0.9413
Epoch 50/200
1071/1070 [==============================] - 422s 394ms/step - loss: 0.3310 - acc: 0.9476 - val_loss: 0.4549 - val_acc: 0.9331
Epoch 51/200
1071/1070 [==============================] - 421s 393ms/step - loss: 0.3303 - acc: 0.9472 - val_loss: 0.4118 - val_acc: 0.9417
Epoch 52/200
1071/1070 [==============================] - 427s 399ms/step - loss: 0.3281 - acc: 0.9478 - val_loss: 0.4336 - val_acc: 0.9346
Epoch 53/200
1071/1070 [==============================] - 409s 382ms/step - loss: 0.3282 - acc: 0.9480 - val_loss: 0.4359 - val_acc: 0.9360
Epoch 54/200
1071/1070 [==============================] - 417s 389ms/step - loss: 0.3263 - acc: 0.9480 - val_loss: 0.4139 - val_acc: 0.9409
Epoch 55/200
1071/1070 [==============================] - 442s 413ms/step - loss: 0.3248 - acc: 0.9483 - val_loss: 0.3930 - val_acc: 0.9454
Epoch 56/200
1071/1070 [==============================] - 403s 376ms/step - loss: 0.3223 - acc: 0.9488 - val_loss: 0.4247 - val_acc: 0.9393
Epoch 57/200
1071/1070 [==============================] - 422s 394ms/step - loss: 0.3220 - acc: 0.9485 - val_loss: 0.4075 - val_acc: 0.9432
Epoch 58/200
1071/1070 [==============================] - 429s 401ms/step - loss: 0.3212 - acc: 0.9489 - val_loss: 0.3947 - val_acc: 0.9456
Epoch 59/200
1071/1070 [==============================] - 423s 395ms/step - loss: 0.3200 - acc: 0.9492 - val_loss: 0.4075 - val_acc: 0.9415
Epoch 60/200
1071/1070 [==============================] - 415s 388ms/step - loss: 0.3192 - acc: 0.9491 - val_loss: 0.4006 - val_acc: 0.9435
Epoch 61/200
1071/1070 [==============================] - 409s 382ms/step - loss: 0.3178 - acc: 0.9494 - val_loss: 0.3885 - val_acc: 0.9467
Epoch 62/200
1071/1070 [==============================] - 424s 396ms/step - loss: 0.3161 - acc: 0.9492 - val_loss: 0.3934 - val_acc: 0.9452
Epoch 63/200
1071/1070 [==============================] - 406s 379ms/step - loss: 0.3161 - acc: 0.9492 - val_loss: 0.3996 - val_acc: 0.9431
Epoch 64/200
1071/1070 [==============================] - 418s 390ms/step - loss: 0.3149 - acc: 0.9495 - val_loss: 0.4089 - val_acc: 0.9407
Epoch 65/200
1071/1070 [==============================] - 429s 401ms/step - loss: 0.3138 - acc: 0.9494 - val_loss: 0.4101 - val_acc: 0.9396
Epoch 66/200
1071/1070 [==============================] - 403s 376ms/step - loss: 0.3118 - acc: 0.9500 - val_loss: 0.3971 - val_acc: 0.9433
Epoch 67/200
1071/1070 [==============================] - 423s 395ms/step - loss: 0.3116 - acc: 0.9498 - val_loss: 0.3926 - val_acc: 0.9435
Epoch 68/200
1071/1070 [==============================] - 416s 388ms/step - loss: 0.3096 - acc: 0.9503 - val_loss: 0.3994 - val_acc: 0.9419
Epoch 69/200
1071/1070 [==============================] - 430s 402ms/step - loss: 0.3079 - acc: 0.9503 - val_loss: 0.3834 - val_acc: 0.9467
Epoch 70/200
1071/1070 [==============================] - 421s 393ms/step - loss: 0.3085 - acc: 0.9501 - val_loss: 0.4335 - val_acc: 0.9355
Epoch 71/200
1071/1070 [==============================] - 417s 389ms/step - loss: 0.3069 - acc: 0.9505 - val_loss: 0.4524 - val_acc: 0.9294
Epoch 72/200
1071/1070 [==============================] - 407s 380ms/step - loss: 0.3055 - acc: 0.9510 - val_loss: 0.3944 - val_acc: 0.9430
Epoch 73/200
1071/1070 [==============================] - 431s 402ms/step - loss: 0.3045 - acc: 0.9510 - val_loss: 0.4013 - val_acc: 0.9421
Epoch 74/200
1071/1070 [==============================] - 441s 412ms/step - loss: 0.3044 - acc: 0.9509 - val_loss: 0.3997 - val_acc: 0.9428
Epoch 75/200
1071/1070 [==============================] - 414s 386ms/step - loss: 0.3036 - acc: 0.9508 - val_loss: 0.4018 - val_acc: 0.9415
Epoch 76/200
1071/1070 [==============================] - 409s 382ms/step - loss: 0.3030 - acc: 0.9508 - val_loss: 0.4002 - val_acc: 0.9422
Epoch 77/200
1071/1070 [==============================] - 404s 377ms/step - loss: 0.3014 - acc: 0.9515 - val_loss: 0.3830 - val_acc: 0.9471
Epoch 78/200
1071/1070 [==============================] - 404s 377ms/step - loss: 0.3008 - acc: 0.9513 - val_loss: 0.3920 - val_acc: 0.9444
Epoch 79/200
1071/1070 [==============================] - 394s 368ms/step - loss: 0.3014 - acc: 0.9511 - val_loss: 0.3895 - val_acc: 0.9434
Epoch 80/200
1071/1070 [==============================] - 407s 380ms/step - loss: 0.2997 - acc: 0.9511 - val_loss: 0.3838 - val_acc: 0.9467
Epoch 81/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.2983 - acc: 0.9515 - val_loss: 0.3905 - val_acc: 0.9441
Epoch 82/200
1071/1070 [==============================] - 410s 383ms/step - loss: 0.2411 - acc: 0.9673 - val_loss: 0.3125 - val_acc: 0.9620
Epoch 83/200
1071/1070 [==============================] - 399s 373ms/step - loss: 0.2210 - acc: 0.9712 - val_loss: 0.3057 - val_acc: 0.9629
Epoch 84/200
1071/1070 [==============================] - 424s 396ms/step - loss: 0.2104 - acc: 0.9728 - val_loss: 0.2978 - val_acc: 0.9636
Epoch 85/200
1071/1070 [==============================] - 396s 370ms/step - loss: 0.2024 - acc: 0.9735 - val_loss: 0.2938 - val_acc: 0.9631
Epoch 86/200
1071/1070 [==============================] - 406s 379ms/step - loss: 0.1950 - acc: 0.9741 - val_loss: 0.2902 - val_acc: 0.9626
Epoch 87/200
1071/1070 [==============================] - 413s 386ms/step - loss: 0.1888 - acc: 0.9747 - val_loss: 0.2844 - val_acc: 0.9633
Epoch 88/200
1071/1070 [==============================] - 416s 389ms/step - loss: 0.1838 - acc: 0.9749 - val_loss: 0.2802 - val_acc: 0.9635
Epoch 89/200
1071/1070 [==============================] - 402s 376ms/step - loss: 0.1788 - acc: 0.9753 - val_loss: 0.2796 - val_acc: 0.9633
Epoch 90/200
1071/1070 [==============================] - 422s 394ms/step - loss: 0.1748 - acc: 0.9755 - val_loss: 0.2744 - val_acc: 0.9636
Epoch 91/200
1071/1070 [==============================] - 416s 388ms/step - loss: 0.1713 - acc: 0.9757 - val_loss: 0.2704 - val_acc: 0.9643
Epoch 92/200
1071/1070 [==============================] - 408s 381ms/step - loss: 0.1676 - acc: 0.9760 - val_loss: 0.2686 - val_acc: 0.9642
Epoch 93/200
1071/1070 [==============================] - 411s 384ms/step - loss: 0.1651 - acc: 0.9759 - val_loss: 0.2668 - val_acc: 0.9637
Epoch 94/200
1071/1070 [==============================] - 410s 383ms/step - loss: 0.1608 - acc: 0.9766 - val_loss: 0.2632 - val_acc: 0.9639
Epoch 95/200
1071/1070 [==============================] - 398s 372ms/step - loss: 0.1579 - acc: 0.9768 - val_loss: 0.2645 - val_acc: 0.9634
Epoch 96/200
1071/1070 [==============================] - 413s 386ms/step - loss: 0.1564 - acc: 0.9767 - val_loss: 0.2642 - val_acc: 0.9630
Epoch 97/200
1071/1070 [==============================] - 408s 381ms/step - loss: 0.1538 - acc: 0.9767 - val_loss: 0.2595 - val_acc: 0.9642
Epoch 98/200
1071/1070 [==============================] - 388s 362ms/step - loss: 0.1510 - acc: 0.9770 - val_loss: 0.2580 - val_acc: 0.9642
Epoch 99/200
1071/1070 [==============================] - 428s 400ms/step - loss: 0.1497 - acc: 0.9768 - val_loss: 0.2564 - val_acc: 0.9639
Epoch 100/200
1071/1070 [==============================] - 417s 389ms/step - loss: 0.1468 - acc: 0.9776 - val_loss: 0.2554 - val_acc: 0.9639
Epoch 101/200
1071/1070 [==============================] - 426s 398ms/step - loss: 0.1454 - acc: 0.9774 - val_loss: 0.2536 - val_acc: 0.9633
Epoch 102/200
1071/1070 [==============================] - 402s 376ms/step - loss: 0.1432 - acc: 0.9773 - val_loss: 0.2530 - val_acc: 0.9641
Epoch 103/200
1071/1070 [==============================] - 412s 385ms/step - loss: 0.1418 - acc: 0.9775 - val_loss: 0.2519 - val_acc: 0.9639
Epoch 104/200
1071/1070 [==============================] - 413s 385ms/step - loss: 0.1399 - acc: 0.9777 - val_loss: 0.2511 - val_acc: 0.9640
Epoch 105/200
1071/1070 [==============================] - 415s 387ms/step - loss: 0.1389 - acc: 0.9778 - val_loss: 0.2483 - val_acc: 0.9638
Epoch 106/200
1071/1070 [==============================] - 433s 404ms/step - loss: 0.1378 - acc: 0.9778 - val_loss: 0.2462 - val_acc: 0.9640
Epoch 107/200
1071/1070 [==============================] - 417s 390ms/step - loss: 0.1365 - acc: 0.9779 - val_loss: 0.2520 - val_acc: 0.9626
Epoch 108/200
1071/1070 [==============================] - 404s 377ms/step - loss: 0.1347 - acc: 0.9780 - val_loss: 0.2447 - val_acc: 0.9642
Epoch 109/200
1071/1070 [==============================] - 398s 372ms/step - loss: 0.1342 - acc: 0.9778 - val_loss: 0.2503 - val_acc: 0.9633
Epoch 110/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.1326 - acc: 0.9780 - val_loss: 0.2443 - val_acc: 0.9638
Epoch 111/200
1071/1070 [==============================] - 404s 377ms/step - loss: 0.1312 - acc: 0.9781 - val_loss: 0.2444 - val_acc: 0.9633
Epoch 112/200
1071/1070 [==============================] - 408s 381ms/step - loss: 0.1301 - acc: 0.9783 - val_loss: 0.2457 - val_acc: 0.9636
Epoch 113/200
1071/1070 [==============================] - 409s 382ms/step - loss: 0.1296 - acc: 0.9781 - val_loss: 0.2440 - val_acc: 0.9637
Epoch 114/200
1071/1070 [==============================] - 417s 390ms/step - loss: 0.1279 - acc: 0.9784 - val_loss: 0.2428 - val_acc: 0.9638
Epoch 115/200
1071/1070 [==============================] - 404s 377ms/step - loss: 0.1268 - acc: 0.9787 - val_loss: 0.2443 - val_acc: 0.9634
Epoch 116/200
1071/1070 [==============================] - 412s 385ms/step - loss: 0.1266 - acc: 0.9784 - val_loss: 0.2435 - val_acc: 0.9637
Epoch 117/200
1071/1070 [==============================] - 412s 385ms/step - loss: 0.1259 - acc: 0.9787 - val_loss: 0.2444 - val_acc: 0.9634
Epoch 118/200
1071/1070 [==============================] - 418s 390ms/step - loss: 0.1244 - acc: 0.9786 - val_loss: 0.2433 - val_acc: 0.9635
Epoch 119/200
1071/1070 [==============================] - 438s 409ms/step - loss: 0.1245 - acc: 0.9785 - val_loss: 0.2415 - val_acc: 0.9640
Epoch 120/200
1071/1070 [==============================] - 405s 378ms/step - loss: 0.1229 - acc: 0.9788 - val_loss: 0.2404 - val_acc: 0.9638
Epoch 121/200
1071/1070 [==============================] - 423s 395ms/step - loss: 0.1233 - acc: 0.9786 - val_loss: 0.2420 - val_acc: 0.9628
Epoch 122/200
1071/1070 [==============================] - 429s 401ms/step - loss: 0.1150 - acc: 0.9811 - val_loss: 0.2320 - val_acc: 0.9653
Epoch 123/200
1071/1070 [==============================] - 426s 397ms/step - loss: 0.1126 - acc: 0.9818 - val_loss: 0.2314 - val_acc: 0.9655
Epoch 124/200
1071/1070 [==============================] - 408s 381ms/step - loss: 0.1115 - acc: 0.9821 - val_loss: 0.2312 - val_acc: 0.9655
Epoch 125/200
1071/1070 [==============================] - 427s 399ms/step - loss: 0.1114 - acc: 0.9819 - val_loss: 0.2317 - val_acc: 0.9655
Epoch 126/200
1071/1070 [==============================] - 446s 416ms/step - loss: 0.1100 - acc: 0.9825 - val_loss: 0.2306 - val_acc: 0.9657
Epoch 127/200
1071/1070 [==============================] - 419s 391ms/step - loss: 0.1099 - acc: 0.9825 - val_loss: 0.2301 - val_acc: 0.9659
Epoch 128/200
1071/1070 [==============================] - 410s 383ms/step - loss: 0.1088 - acc: 0.9827 - val_loss: 0.2297 - val_acc: 0.9659
Epoch 129/200
1071/1070 [==============================] - 418s 390ms/step - loss: 0.1091 - acc: 0.9826 - val_loss: 0.2302 - val_acc: 0.9659
Epoch 130/200
1071/1070 [==============================] - 416s 388ms/step - loss: 0.1087 - acc: 0.9828 - val_loss: 0.2302 - val_acc: 0.9659
Epoch 131/200
1071/1070 [==============================] - 391s 365ms/step - loss: 0.1077 - acc: 0.9832 - val_loss: 0.2300 - val_acc: 0.9660
Epoch 132/200
1071/1070 [==============================] - 431s 402ms/step - loss: 0.1068 - acc: 0.9832 - val_loss: 0.2303 - val_acc: 0.9658
Epoch 133/200
1071/1070 [==============================] - 404s 377ms/step - loss: 0.1069 - acc: 0.9831 - val_loss: 0.2296 - val_acc: 0.9660
Epoch 134/200
1071/1070 [==============================] - 412s 385ms/step - loss: 0.1068 - acc: 0.9832 - val_loss: 0.2298 - val_acc: 0.9660
Epoch 135/200
1071/1070 [==============================] - 417s 389ms/step - loss: 0.1061 - acc: 0.9831 - val_loss: 0.2301 - val_acc: 0.9659
Epoch 136/200
1071/1070 [==============================] - 436s 408ms/step - loss: 0.1061 - acc: 0.9833 - val_loss: 0.2284 - val_acc: 0.9663
Epoch 137/200
1071/1070 [==============================] - 414s 387ms/step - loss: 0.1052 - acc: 0.9835 - val_loss: 0.2291 - val_acc: 0.9665
Epoch 138/200
1071/1070 [==============================] - 412s 385ms/step - loss: 0.1057 - acc: 0.9834 - val_loss: 0.2296 - val_acc: 0.9659
Epoch 139/200
1071/1070 [==============================] - 427s 399ms/step - loss: 0.1055 - acc: 0.9834 - val_loss: 0.2294 - val_acc: 0.9659
Epoch 140/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.1052 - acc: 0.9836 - val_loss: 0.2290 - val_acc: 0.9661
Epoch 141/200
1071/1070 [==============================] - 438s 409ms/step - loss: 0.1053 - acc: 0.9833 - val_loss: 0.2289 - val_acc: 0.9660
Epoch 142/200
1071/1070 [==============================] - 437s 408ms/step - loss: 0.1045 - acc: 0.9836 - val_loss: 0.2290 - val_acc: 0.9660
Epoch 143/200
1071/1070 [==============================] - 423s 395ms/step - loss: 0.1040 - acc: 0.9836 - val_loss: 0.2290 - val_acc: 0.9662
Epoch 144/200
1071/1070 [==============================] - 425s 397ms/step - loss: 0.1048 - acc: 0.9834 - val_loss: 0.2291 - val_acc: 0.9659
Epoch 145/200
1071/1070 [==============================] - 439s 410ms/step - loss: 0.1036 - acc: 0.9838 - val_loss: 0.2290 - val_acc: 0.9660
Epoch 146/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.1035 - acc: 0.9836 - val_loss: 0.2291 - val_acc: 0.9660
Epoch 147/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.1030 - acc: 0.9837 - val_loss: 0.2285 - val_acc: 0.9662
Epoch 148/200
1071/1070 [==============================] - 430s 401ms/step - loss: 0.1028 - acc: 0.9839 - val_loss: 0.2292 - val_acc: 0.9660
Epoch 149/200
1071/1070 [==============================] - 426s 398ms/step - loss: 0.1029 - acc: 0.9837 - val_loss: 0.2283 - val_acc: 0.9662
Epoch 150/200
1071/1070 [==============================] - 427s 399ms/step - loss: 0.1027 - acc: 0.9838 - val_loss: 0.2284 - val_acc: 0.9662
Epoch 151/200
1071/1070 [==============================] - 416s 388ms/step - loss: 0.1026 - acc: 0.9838 - val_loss: 0.2276 - val_acc: 0.9663
Epoch 152/200
1071/1070 [==============================] - 454s 424ms/step - loss: 0.1022 - acc: 0.9839 - val_loss: 0.2284 - val_acc: 0.9660
Epoch 153/200
1071/1070 [==============================] - 415s 387ms/step - loss: 0.1017 - acc: 0.9840 - val_loss: 0.2276 - val_acc: 0.9663
Epoch 154/200
1071/1070 [==============================] - 425s 396ms/step - loss: 0.1024 - acc: 0.9838 - val_loss: 0.2276 - val_acc: 0.9662
Epoch 155/200
1071/1070 [==============================] - 410s 383ms/step - loss: 0.1016 - acc: 0.9840 - val_loss: 0.2275 - val_acc: 0.9661
Epoch 156/200
1071/1070 [==============================] - 409s 382ms/step - loss: 0.1011 - acc: 0.9841 - val_loss: 0.2274 - val_acc: 0.9662
Epoch 157/200
1071/1070 [==============================] - 419s 392ms/step - loss: 0.1015 - acc: 0.9838 - val_loss: 0.2273 - val_acc: 0.9662
Epoch 158/200
1071/1070 [==============================] - 435s 406ms/step - loss: 0.1010 - acc: 0.9842 - val_loss: 0.2280 - val_acc: 0.9660
Epoch 159/200
1071/1070 [==============================] - 406s 379ms/step - loss: 0.1015 - acc: 0.9840 - val_loss: 0.2271 - val_acc: 0.9663
Epoch 160/200
1071/1070 [==============================] - 409s 381ms/step - loss: 0.1005 - acc: 0.9842 - val_loss: 0.2267 - val_acc: 0.9665
Epoch 161/200
1071/1070 [==============================] - 427s 399ms/step - loss: 0.1004 - acc: 0.9842 - val_loss: 0.2273 - val_acc: 0.9664
Epoch 162/200
1071/1070 [==============================] - 417s 390ms/step - loss: 0.0997 - acc: 0.9843 - val_loss: 0.2269 - val_acc: 0.9665
Epoch 163/200
1071/1070 [==============================] - 413s 385ms/step - loss: 0.0998 - acc: 0.9844 - val_loss: 0.2268 - val_acc: 0.9666
Epoch 164/200
1071/1070 [==============================] - 419s 391ms/step - loss: 0.1000 - acc: 0.9844 - val_loss: 0.2270 - val_acc: 0.9666
Epoch 165/200
1071/1070 [==============================] - 443s 414ms/step - loss: 0.0999 - acc: 0.9845 - val_loss: 0.2269 - val_acc: 0.9665
Epoch 166/200
1071/1070 [==============================] - 414s 387ms/step - loss: 0.0997 - acc: 0.9844 - val_loss: 0.2270 - val_acc: 0.9665
Epoch 167/200
1071/1070 [==============================] - 412s 385ms/step - loss: 0.0994 - acc: 0.9845 - val_loss: 0.2271 - val_acc: 0.9664
Epoch 168/200
1071/1070 [==============================] - 407s 380ms/step - loss: 0.0992 - acc: 0.9845 - val_loss: 0.2269 - val_acc: 0.9666
Epoch 169/200
1071/1070 [==============================] - 429s 400ms/step - loss: 0.0994 - acc: 0.9844 - val_loss: 0.2270 - val_acc: 0.9665
Epoch 170/200
1071/1070 [==============================] - 419s 391ms/step - loss: 0.0992 - acc: 0.9846 - val_loss: 0.2271 - val_acc: 0.9666
Epoch 171/200
1071/1070 [==============================] - 430s 401ms/step - loss: 0.0992 - acc: 0.9845 - val_loss: 0.2270 - val_acc: 0.9665
Epoch 172/200
1071/1070 [==============================] - 410s 383ms/step - loss: 0.0994 - acc: 0.9845 - val_loss: 0.2269 - val_acc: 0.9665
Epoch 173/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.0991 - acc: 0.9847 - val_loss: 0.2269 - val_acc: 0.9666
Epoch 174/200
1071/1070 [==============================] - 412s 384ms/step - loss: 0.0992 - acc: 0.9845 - val_loss: 0.2267 - val_acc: 0.9665
Epoch 175/200
1071/1070 [==============================] - 420s 392ms/step - loss: 0.0992 - acc: 0.9844 - val_loss: 0.2267 - val_acc: 0.9666
Epoch 176/200
1071/1070 [==============================] - 413s 386ms/step - loss: 0.0996 - acc: 0.9843 - val_loss: 0.2267 - val_acc: 0.9665
Epoch 177/200
1071/1070 [==============================] - 425s 396ms/step - loss: 0.0990 - acc: 0.9845 - val_loss: 0.2269 - val_acc: 0.9665
Epoch 178/200
1071/1070 [==============================] - 420s 393ms/step - loss: 0.0992 - acc: 0.9846 - val_loss: 0.2267 - val_acc: 0.9665
Epoch 179/200
1071/1070 [==============================] - 418s 390ms/step - loss: 0.0986 - acc: 0.9848 - val_loss: 0.2268 - val_acc: 0.9665
Epoch 180/200
1071/1070 [==============================] - 393s 367ms/step - loss: 0.0989 - acc: 0.9844 - val_loss: 0.2266 - val_acc: 0.9665
Epoch 181/200
1071/1070 [==============================] - 436s 407ms/step - loss: 0.0994 - acc: 0.9845 - val_loss: 0.2266 - val_acc: 0.9664
Epoch 182/200
1071/1070 [==============================] - 428s 400ms/step - loss: 0.0989 - acc: 0.9845 - val_loss: 0.2267 - val_acc: 0.9665
Epoch 183/200
1071/1070 [==============================] - 409s 382ms/step - loss: 0.0994 - acc: 0.9845 - val_loss: 0.2266 - val_acc: 0.9665
Epoch 184/200
1071/1070 [==============================] - 421s 393ms/step - loss: 0.0996 - acc: 0.9844 - val_loss: 0.2264 - val_acc: 0.9665
Epoch 185/200
1071/1070 [==============================] - 435s 406ms/step - loss: 0.0992 - acc: 0.9844 - val_loss: 0.2266 - val_acc: 0.9664
Epoch 186/200
1071/1070 [==============================] - 421s 393ms/step - loss: 0.0989 - acc: 0.9845 - val_loss: 0.2266 - val_acc: 0.9665
Epoch 187/200
1071/1070 [==============================] - 434s 406ms/step - loss: 0.0991 - acc: 0.9844 - val_loss: 0.2265 - val_acc: 0.9664
Epoch 188/200
1071/1070 [==============================] - 423s 395ms/step - loss: 0.0988 - acc: 0.9846 - val_loss: 0.2265 - val_acc: 0.9665
Epoch 189/200
1071/1070 [==============================] - 423s 395ms/step - loss: 0.0990 - acc: 0.9846 - val_loss: 0.2264 - val_acc: 0.9665
Epoch 190/200
1071/1070 [==============================] - 414s 387ms/step - loss: 0.0987 - acc: 0.9847 - val_loss: 0.2263 - val_acc: 0.9665
Epoch 191/200
1071/1070 [==============================] - 424s 396ms/step - loss: 0.0990 - acc: 0.9846 - val_loss: 0.2264 - val_acc: 0.9666
Epoch 192/200
1071/1070 [==============================] - 426s 397ms/step - loss: 0.0988 - acc: 0.9846 - val_loss: 0.2266 - val_acc: 0.9665
Epoch 193/200
1071/1070 [==============================] - 417s 389ms/step - loss: 0.0986 - acc: 0.9846 - val_loss: 0.2264 - val_acc: 0.9665
Epoch 194/200
1071/1070 [==============================] - 437s 408ms/step - loss: 0.0993 - acc: 0.9845 - val_loss: 0.2265 - val_acc: 0.9665
Epoch 195/200
1071/1070 [==============================] - 425s 397ms/step - loss: 0.0986 - acc: 0.9847 - val_loss: 0.2265 - val_acc: 0.9664
Epoch 196/200
1071/1070 [==============================] - 413s 385ms/step - loss: 0.0995 - acc: 0.9845 - val_loss: 0.2267 - val_acc: 0.9663
Epoch 197/200
1071/1070 [==============================] - 420s 393ms/step - loss: 0.0988 - acc: 0.9845 - val_loss: 0.2264 - val_acc: 0.9665
Epoch 198/200
1071/1070 [==============================] - 429s 400ms/step - loss: 0.0991 - acc: 0.9846 - val_loss: 0.2265 - val_acc: 0.9665
Epoch 199/200
1071/1070 [==============================] - 414s 387ms/step - loss: 0.0990 - acc: 0.9847 - val_loss: 0.2266 - val_acc: 0.9664
Epoch 200/200
1071/1070 [==============================] - 439s 410ms/step - loss: 0.0990 - acc: 0.9845 - val_loss: 0.2267 - val_acc: 0.9664
Saved trained model at .\result_recog\test191002_aspect_ver9\my_resnet\trained_model.h5
Traceback (most recent call last):
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\kuzushiji_project2.py", line 97, in <module>
    pipe_line.ResNetPipeline_191002AspectVer9().run_train()
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\pipe_line.py", line 7334, in run_train
    pred_tr_letter_nos = self.__predict_using_input(tr_inputs)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\pipe_line.py", line 7375, in __predict_using_input
    soft=soft)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\char_classification\resnet.py", line 1748, in predict_tta
    auged_imgs_set = tta_func(images, other_inputs)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\char_classification\tta.py", line 42, in augment_image
    auged_imgs = self.__translate(images, w_sft, h_sft)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\char_classification\tta.py", line 63, in __translate
    translated_images = np.pad(images, pad_width=(pad_w_0, pad_w_1, pad_w_2, pad_w_3), mode='edge')
  File "C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\numpy\lib\arraypad.py", line 1252, in pad
    newmat = _append_edge(newmat, pad_after, axis)
  File "C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\numpy\lib\arraypad.py", line 218, in _append_edge
    return _do_append(arr, edge_arr.repeat(pad_amt, axis=axis), axis)
  File "C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\numpy\lib\arraypad.py", line 104, in _do_append
    (arr, pad_chunk.astype(arr.dtype, copy=False)), axis=axis)
MemoryError
続行するには何かキーを押してください . . .