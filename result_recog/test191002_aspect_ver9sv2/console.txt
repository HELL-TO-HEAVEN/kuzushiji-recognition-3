Using TensorFlow backend.
 read image 3881/3104
 read image 3880/777
WARNING: Logging before flag parsing goes to stderr.
W1006 16:27:16.265036 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W1006 16:27:16.593086 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W1006 16:27:16.718056 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

W1006 16:27:16.827429 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

W1006 16:27:16.827429 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2019-10-06 16:27:16.850359: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-10-06 16:27:16.864149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-10-06 16:27:17.019193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
2019-10-06 16:27:17.023537: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-10-06 16:27:17.028580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-06 16:27:19.072796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-06 16:27:19.078402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-10-06 16:27:19.081655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-10-06 16:27:19.090881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6351 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
W1006 16:27:19.566260 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\backend\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

W1006 16:27:21.940656 14164 deprecation_wrapper.py:119] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\keras\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 64, 64, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 48)   480         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 48)   0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 16, 16, 96)   41568       activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 16, 16, 96)   384         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 16, 16, 96)   0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 16, 16, 96)   83040       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 16, 16, 96)   384         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 16, 16, 96)   0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 16, 16, 96)   83040       activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 16, 16, 96)   384         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 16, 16, 96)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 16, 16, 96)   83040       activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 16, 16, 96)   384         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 16, 16, 96)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 16, 16, 96)   83040       activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 16, 16, 96)   384         conv2d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 16, 16, 96)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 96)   83040       activation_6[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 96)   4704        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 96)   384         conv2d_7[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 16, 16, 96)   0           conv2d_8[0][0]
                                                                 batch_normalization_7[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 8, 8, 192)    166080      add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 8, 8, 192)    768         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 8, 8, 192)    0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 8, 8, 192)    331968      activation_7[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 8, 8, 192)    768         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 8, 8, 192)    0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 8, 8, 192)    331968      activation_8[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 8, 8, 192)    768         conv2d_11[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 8, 8, 192)    0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 8, 8, 192)    331968      activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 8, 8, 192)    768         conv2d_12[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 8, 8, 192)    0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 8, 8, 192)    331968      activation_10[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 8, 8, 192)    768         conv2d_13[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 8, 8, 192)    0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 8, 8, 192)    331968      activation_11[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 192)    768         conv2d_14[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 8, 8, 192)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 192)    331968      activation_12[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 8, 8, 192)    768         conv2d_15[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 192)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 8, 8, 192)    331968      activation_13[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 8, 8, 192)    18624       add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 8, 8, 192)    768         conv2d_16[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 8, 8, 192)    0           conv2d_17[0][0]
                                                                 batch_normalization_15[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 4, 4, 384)    663936      add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 4, 4, 384)    1536        conv2d_18[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 4, 4, 384)    0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 4, 4, 384)    1327488     activation_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 4, 4, 384)    1536        conv2d_19[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 4, 4, 384)    0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 4, 4, 384)    1327488     activation_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 4, 4, 384)    1536        conv2d_20[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 4, 4, 384)    0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 4, 4, 384)    1327488     activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 4, 4, 384)    1536        conv2d_21[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 4, 4, 384)    0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 4, 4, 384)    1327488     activation_17[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 4, 4, 384)    1536        conv2d_22[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 4, 4, 384)    0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 4, 4, 384)    1327488     activation_18[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 4, 4, 384)    74112       add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 4, 4, 384)    1536        conv2d_23[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 4, 4, 384)    0           conv2d_24[0][0]
                                                                 batch_normalization_21[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 4, 4, 384)    1536        add_3[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 4, 4, 384)    0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 1)            0
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 384)          0           activation_19[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 1)            4           input_2[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 385)          0           global_average_pooling2d_1[0][0]
                                                                 batch_normalization_23[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 4212)         1625832     concatenate_1[0][0]
==================================================================================================
Total params: 11,991,148
Trainable params: 11,981,450
Non-trainable params: 9,698
__________________________________________________________________________________________________
W1006 16:27:22.409304 14164 deprecation.py:323] From C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/200
1073/1072 [==============================] - 554s 516ms/step - loss: 2.3576 - acc: 0.6958 - val_loss: 1.3849 - val_acc: 0.8173
Epoch 2/200
1073/1072 [==============================] - 743s 692ms/step - loss: 1.1474 - acc: 0.8422 - val_loss: 1.0861 - val_acc: 0.8515
Epoch 3/200
1073/1072 [==============================] - 721s 672ms/step - loss: 0.9233 - acc: 0.8716 - val_loss: 0.9170 - val_acc: 0.8758
Epoch 4/200
1073/1072 [==============================] - 682s 636ms/step - loss: 0.8192 - acc: 0.8851 - val_loss: 0.8319 - val_acc: 0.8901
Epoch 5/200
1073/1072 [==============================] - 709s 661ms/step - loss: 0.7491 - acc: 0.8949 - val_loss: 0.7547 - val_acc: 0.9017
Epoch 6/200
1073/1072 [==============================] - 721s 672ms/step - loss: 0.6920 - acc: 0.9025 - val_loss: 0.7515 - val_acc: 0.8948
Epoch 7/200
1073/1072 [==============================] - 688s 641ms/step - loss: 0.6496 - acc: 0.9086 - val_loss: 0.6776 - val_acc: 0.9108
Epoch 8/200
1073/1072 [==============================] - 702s 655ms/step - loss: 0.6133 - acc: 0.9124 - val_loss: 0.6406 - val_acc: 0.9154
Epoch 9/200
1073/1072 [==============================] - 704s 656ms/step - loss: 0.5824 - acc: 0.9164 - val_loss: 0.6180 - val_acc: 0.9188
Epoch 10/200
1073/1072 [==============================] - 698s 651ms/step - loss: 0.5588 - acc: 0.9192 - val_loss: 0.6542 - val_acc: 0.9080
Epoch 11/200
1073/1072 [==============================] - 703s 655ms/step - loss: 0.5382 - acc: 0.9222 - val_loss: 0.5806 - val_acc: 0.9224
Epoch 12/200
1073/1072 [==============================] - 679s 633ms/step - loss: 0.5179 - acc: 0.9245 - val_loss: 0.5721 - val_acc: 0.9241
Epoch 13/200
1073/1072 [==============================] - 694s 647ms/step - loss: 0.5020 - acc: 0.9266 - val_loss: 0.5528 - val_acc: 0.9269
Epoch 14/200
1073/1072 [==============================] - 681s 635ms/step - loss: 0.4892 - acc: 0.9275 - val_loss: 0.5397 - val_acc: 0.9296
Epoch 15/200
1073/1072 [==============================] - 634s 590ms/step - loss: 0.4740 - acc: 0.9302 - val_loss: 0.5370 - val_acc: 0.9283
Epoch 16/200
1073/1072 [==============================] - 660s 615ms/step - loss: 0.4650 - acc: 0.9310 - val_loss: 0.5078 - val_acc: 0.9346
Epoch 17/200
1073/1072 [==============================] - 670s 625ms/step - loss: 0.4552 - acc: 0.9320 - val_loss: 0.5240 - val_acc: 0.9276
Epoch 18/200
1073/1072 [==============================] - 683s 636ms/step - loss: 0.4461 - acc: 0.9336 - val_loss: 0.5113 - val_acc: 0.9317
Epoch 19/200
1073/1072 [==============================] - 670s 624ms/step - loss: 0.4375 - acc: 0.9344 - val_loss: 0.4837 - val_acc: 0.9357
Epoch 20/200
1073/1072 [==============================] - 746s 696ms/step - loss: 0.4316 - acc: 0.9349 - val_loss: 0.4877 - val_acc: 0.9344
Epoch 21/200
1073/1072 [==============================] - 657s 612ms/step - loss: 0.4235 - acc: 0.9361 - val_loss: 0.4735 - val_acc: 0.9364
Epoch 22/200
1073/1072 [==============================] - 669s 624ms/step - loss: 0.4177 - acc: 0.9368 - val_loss: 0.4900 - val_acc: 0.9327
Epoch 23/200
1073/1072 [==============================] - 711s 663ms/step - loss: 0.4120 - acc: 0.9373 - val_loss: 0.4766 - val_acc: 0.9343
Epoch 24/200
1073/1072 [==============================] - 662s 617ms/step - loss: 0.4075 - acc: 0.9381 - val_loss: 0.4840 - val_acc: 0.9351
Epoch 25/200
1073/1072 [==============================] - 674s 629ms/step - loss: 0.4020 - acc: 0.9389 - val_loss: 0.4943 - val_acc: 0.9296
Epoch 26/200
1073/1072 [==============================] - 693s 646ms/step - loss: 0.3983 - acc: 0.9390 - val_loss: 0.4805 - val_acc: 0.9319
Epoch 27/200
1073/1072 [==============================] - 645s 601ms/step - loss: 0.3928 - acc: 0.9403 - val_loss: 0.4609 - val_acc: 0.9372
Epoch 28/200
1073/1072 [==============================] - 714s 665ms/step - loss: 0.3894 - acc: 0.9398 - val_loss: 0.4416 - val_acc: 0.9422
Epoch 29/200
1073/1072 [==============================] - 630s 587ms/step - loss: 0.3858 - acc: 0.9406 - val_loss: 0.4549 - val_acc: 0.9395
Epoch 30/200
1073/1072 [==============================] - 620s 578ms/step - loss: 0.3827 - acc: 0.9408 - val_loss: 0.4609 - val_acc: 0.9356
Epoch 31/200
1073/1072 [==============================] - 646s 602ms/step - loss: 0.3770 - acc: 0.9421 - val_loss: 0.4583 - val_acc: 0.9367
Epoch 32/200
1073/1072 [==============================] - 608s 566ms/step - loss: 0.3742 - acc: 0.9424 - val_loss: 0.4748 - val_acc: 0.9320
Epoch 33/200
1073/1072 [==============================] - 615s 573ms/step - loss: 0.3710 - acc: 0.9428 - val_loss: 0.4380 - val_acc: 0.9409
Epoch 34/200
1073/1072 [==============================] - 604s 562ms/step - loss: 0.3674 - acc: 0.9431 - val_loss: 0.4537 - val_acc: 0.9367
Epoch 35/200
1073/1072 [==============================] - 617s 575ms/step - loss: 0.3670 - acc: 0.9428 - val_loss: 0.4578 - val_acc: 0.9354
Epoch 36/200
1073/1072 [==============================] - 682s 635ms/step - loss: 0.3623 - acc: 0.9437 - val_loss: 0.4351 - val_acc: 0.9403
Epoch 37/200
1073/1072 [==============================] - 581s 541ms/step - loss: 0.3608 - acc: 0.9440 - val_loss: 0.4320 - val_acc: 0.9413
Epoch 38/200
1073/1072 [==============================] - 572s 533ms/step - loss: 0.3582 - acc: 0.9441 - val_loss: 0.4335 - val_acc: 0.9412
Epoch 39/200
1073/1072 [==============================] - 586s 546ms/step - loss: 0.3556 - acc: 0.9442 - val_loss: 0.4631 - val_acc: 0.9336
Epoch 40/200
1073/1072 [==============================] - 556s 518ms/step - loss: 0.3523 - acc: 0.9451 - val_loss: 0.4323 - val_acc: 0.9397
Epoch 41/200
1073/1072 [==============================] - 565s 526ms/step - loss: 0.3507 - acc: 0.9453 - val_loss: 0.4332 - val_acc: 0.9391
Epoch 42/200
1073/1072 [==============================] - 539s 503ms/step - loss: 0.3487 - acc: 0.9449 - val_loss: 0.4476 - val_acc: 0.9355
Epoch 43/200
1073/1072 [==============================] - 560s 522ms/step - loss: 0.3458 - acc: 0.9458 - val_loss: 0.4298 - val_acc: 0.9410
Epoch 44/200
1073/1072 [==============================] - 521s 486ms/step - loss: 0.3435 - acc: 0.9459 - val_loss: 0.4474 - val_acc: 0.9364
Epoch 45/200
1073/1072 [==============================] - 528s 492ms/step - loss: 0.3423 - acc: 0.9459 - val_loss: 0.4215 - val_acc: 0.9415
Epoch 46/200
1073/1072 [==============================] - 514s 479ms/step - loss: 0.3406 - acc: 0.9462 - val_loss: 0.4073 - val_acc: 0.9455
Epoch 47/200
1073/1072 [==============================] - 546s 509ms/step - loss: 0.3393 - acc: 0.9459 - val_loss: 0.4406 - val_acc: 0.9370
Epoch 48/200
1073/1072 [==============================] - 525s 489ms/step - loss: 0.3365 - acc: 0.9463 - val_loss: 0.4331 - val_acc: 0.9369
Epoch 49/200
1073/1072 [==============================] - 560s 522ms/step - loss: 0.3352 - acc: 0.9470 - val_loss: 0.4382 - val_acc: 0.9364
Epoch 50/200
1073/1072 [==============================] - 667s 622ms/step - loss: 0.3338 - acc: 0.9470 - val_loss: 0.4198 - val_acc: 0.9410
Epoch 51/200
1073/1072 [==============================] - 652s 608ms/step - loss: 0.3319 - acc: 0.9472 - val_loss: 0.4097 - val_acc: 0.9426
Epoch 52/200
1073/1072 [==============================] - 534s 497ms/step - loss: 0.3288 - acc: 0.9477 - val_loss: 0.4111 - val_acc: 0.9425
Epoch 53/200
1073/1072 [==============================] - 519s 484ms/step - loss: 0.3286 - acc: 0.9476 - val_loss: 0.4299 - val_acc: 0.9398
Epoch 54/200
1073/1072 [==============================] - 516s 481ms/step - loss: 0.3269 - acc: 0.9478 - val_loss: 0.3948 - val_acc: 0.9456
Epoch 55/200
1073/1072 [==============================] - 517s 482ms/step - loss: 0.3236 - acc: 0.9484 - val_loss: 0.4180 - val_acc: 0.9401
Epoch 56/200
1073/1072 [==============================] - 514s 479ms/step - loss: 0.3233 - acc: 0.9484 - val_loss: 0.4253 - val_acc: 0.9391
Epoch 57/200
1073/1072 [==============================] - 506s 471ms/step - loss: 0.3246 - acc: 0.9479 - val_loss: 0.4489 - val_acc: 0.9336
Epoch 58/200
1073/1072 [==============================] - 524s 489ms/step - loss: 0.3209 - acc: 0.9484 - val_loss: 0.4132 - val_acc: 0.9415
Epoch 59/200
1073/1072 [==============================] - 512s 478ms/step - loss: 0.3197 - acc: 0.9488 - val_loss: 0.4037 - val_acc: 0.9442
Epoch 60/200
1073/1072 [==============================] - 508s 473ms/step - loss: 0.3197 - acc: 0.9486 - val_loss: 0.4109 - val_acc: 0.9414
Epoch 61/200
1073/1072 [==============================] - 521s 486ms/step - loss: 0.3186 - acc: 0.9487 - val_loss: 0.4131 - val_acc: 0.9399
Epoch 62/200
1073/1072 [==============================] - 527s 491ms/step - loss: 0.3161 - acc: 0.9497 - val_loss: 0.4257 - val_acc: 0.9368
Epoch 63/200
1073/1072 [==============================] - 511s 476ms/step - loss: 0.3140 - acc: 0.9493 - val_loss: 0.4190 - val_acc: 0.9388
Epoch 64/200
1073/1072 [==============================] - 517s 482ms/step - loss: 0.3132 - acc: 0.9497 - val_loss: 0.4117 - val_acc: 0.9424
Epoch 65/200
1073/1072 [==============================] - 521s 485ms/step - loss: 0.3127 - acc: 0.9497 - val_loss: 0.4326 - val_acc: 0.9384
Epoch 66/200
1073/1072 [==============================] - 530s 494ms/step - loss: 0.3118 - acc: 0.9499 - val_loss: 0.3972 - val_acc: 0.9453
Epoch 67/200
1073/1072 [==============================] - 528s 492ms/step - loss: 0.3107 - acc: 0.9498 - val_loss: 0.4177 - val_acc: 0.9422
Epoch 68/200
1073/1072 [==============================] - 524s 489ms/step - loss: 0.3109 - acc: 0.9500 - val_loss: 0.4220 - val_acc: 0.9389
Epoch 69/200
1073/1072 [==============================] - 533s 497ms/step - loss: 0.3091 - acc: 0.9501 - val_loss: 0.3929 - val_acc: 0.9458
Epoch 70/200
1073/1072 [==============================] - 522s 486ms/step - loss: 0.3082 - acc: 0.9504 - val_loss: 0.4093 - val_acc: 0.9422
Epoch 71/200
1073/1072 [==============================] - 525s 489ms/step - loss: 0.3055 - acc: 0.9506 - val_loss: 0.4395 - val_acc: 0.9346
Epoch 72/200
1073/1072 [==============================] - 516s 481ms/step - loss: 0.3048 - acc: 0.9508 - val_loss: 0.4046 - val_acc: 0.9435
Epoch 73/200
1073/1072 [==============================] - 522s 487ms/step - loss: 0.3053 - acc: 0.9503 - val_loss: 0.4091 - val_acc: 0.9417
Epoch 74/200
1073/1072 [==============================] - 521s 485ms/step - loss: 0.3037 - acc: 0.9505 - val_loss: 0.4214 - val_acc: 0.9379
Epoch 75/200
1073/1072 [==============================] - 520s 484ms/step - loss: 0.3032 - acc: 0.9510 - val_loss: 0.4053 - val_acc: 0.9424
Epoch 76/200
1073/1072 [==============================] - 528s 492ms/step - loss: 0.3018 - acc: 0.9509 - val_loss: 0.4124 - val_acc: 0.9419
Epoch 77/200
1073/1072 [==============================] - 529s 493ms/step - loss: 0.3017 - acc: 0.9509 - val_loss: 0.4137 - val_acc: 0.9399
Epoch 78/200
1073/1072 [==============================] - 518s 482ms/step - loss: 0.3005 - acc: 0.9512 - val_loss: 0.4177 - val_acc: 0.9376
Epoch 79/200
1073/1072 [==============================] - 523s 487ms/step - loss: 0.3020 - acc: 0.9507 - val_loss: 0.4035 - val_acc: 0.9421
Epoch 80/200
1073/1072 [==============================] - 513s 478ms/step - loss: 0.2987 - acc: 0.9513 - val_loss: 0.4094 - val_acc: 0.9410
Epoch 81/200
1073/1072 [==============================] - 528s 492ms/step - loss: 0.2971 - acc: 0.9517 - val_loss: 0.4167 - val_acc: 0.9393
Epoch 82/200
1073/1072 [==============================] - 522s 487ms/step - loss: 0.2404 - acc: 0.9671 - val_loss: 0.3216 - val_acc: 0.9613
Epoch 83/200
1073/1072 [==============================] - 552s 514ms/step - loss: 0.2197 - acc: 0.9713 - val_loss: 0.3131 - val_acc: 0.9622
Epoch 84/200
1073/1072 [==============================] - 534s 497ms/step - loss: 0.2095 - acc: 0.9728 - val_loss: 0.3063 - val_acc: 0.9625
Epoch 85/200
1073/1072 [==============================] - 513s 478ms/step - loss: 0.2015 - acc: 0.9733 - val_loss: 0.3019 - val_acc: 0.9625
Epoch 86/200
1073/1072 [==============================] - 522s 487ms/step - loss: 0.1939 - acc: 0.9740 - val_loss: 0.2962 - val_acc: 0.9627
Epoch 87/200
1073/1072 [==============================] - 521s 485ms/step - loss: 0.1877 - acc: 0.9746 - val_loss: 0.2908 - val_acc: 0.9632
Epoch 88/200
1073/1072 [==============================] - 521s 485ms/step - loss: 0.1827 - acc: 0.9747 - val_loss: 0.2869 - val_acc: 0.9637
Epoch 89/200
1073/1072 [==============================] - 514s 479ms/step - loss: 0.1779 - acc: 0.9755 - val_loss: 0.2857 - val_acc: 0.9635
Epoch 90/200
1073/1072 [==============================] - 515s 480ms/step - loss: 0.1731 - acc: 0.9756 - val_loss: 0.2801 - val_acc: 0.9641
Epoch 91/200
1073/1072 [==============================] - 521s 486ms/step - loss: 0.1692 - acc: 0.9760 - val_loss: 0.2800 - val_acc: 0.9632
Epoch 92/200
1073/1072 [==============================] - 512s 477ms/step - loss: 0.1656 - acc: 0.9762 - val_loss: 0.2750 - val_acc: 0.9638
Epoch 93/200
1073/1072 [==============================] - 524s 489ms/step - loss: 0.1616 - acc: 0.9764 - val_loss: 0.2772 - val_acc: 0.9635
Epoch 94/200
1073/1072 [==============================] - 527s 491ms/step - loss: 0.1591 - acc: 0.9766 - val_loss: 0.2734 - val_acc: 0.9630
Epoch 95/200
1073/1072 [==============================] - 521s 486ms/step - loss: 0.1564 - acc: 0.9769 - val_loss: 0.2717 - val_acc: 0.9629
Epoch 96/200
1073/1072 [==============================] - 529s 493ms/step - loss: 0.1532 - acc: 0.9769 - val_loss: 0.2706 - val_acc: 0.9629
Epoch 97/200
1073/1072 [==============================] - 514s 479ms/step - loss: 0.1515 - acc: 0.9770 - val_loss: 0.2646 - val_acc: 0.9639
Epoch 98/200
1073/1072 [==============================] - 512s 477ms/step - loss: 0.1490 - acc: 0.9773 - val_loss: 0.2655 - val_acc: 0.9633
Epoch 99/200
1073/1072 [==============================] - 531s 495ms/step - loss: 0.1471 - acc: 0.9773 - val_loss: 0.2645 - val_acc: 0.9635
Epoch 100/200
1073/1072 [==============================] - 515s 480ms/step - loss: 0.1447 - acc: 0.9776 - val_loss: 0.2626 - val_acc: 0.9635
Epoch 101/200
1073/1072 [==============================] - 506s 471ms/step - loss: 0.1430 - acc: 0.9776 - val_loss: 0.2593 - val_acc: 0.9640
Epoch 102/200
1073/1072 [==============================] - 530s 494ms/step - loss: 0.1409 - acc: 0.9778 - val_loss: 0.2613 - val_acc: 0.9633
Epoch 103/200
1073/1072 [==============================] - 522s 487ms/step - loss: 0.1401 - acc: 0.9776 - val_loss: 0.2596 - val_acc: 0.9631
Epoch 104/200
1073/1072 [==============================] - 527s 492ms/step - loss: 0.1385 - acc: 0.9777 - val_loss: 0.2586 - val_acc: 0.9631
Epoch 105/200
1073/1072 [==============================] - 531s 495ms/step - loss: 0.1373 - acc: 0.9776 - val_loss: 0.2553 - val_acc: 0.9636
Epoch 106/200
1073/1072 [==============================] - 520s 485ms/step - loss: 0.1350 - acc: 0.9781 - val_loss: 0.2559 - val_acc: 0.9635
Epoch 107/200
1073/1072 [==============================] - 515s 480ms/step - loss: 0.1341 - acc: 0.9781 - val_loss: 0.2543 - val_acc: 0.9635
Epoch 108/200
1073/1072 [==============================] - 523s 487ms/step - loss: 0.1319 - acc: 0.9783 - val_loss: 0.2532 - val_acc: 0.9634
Epoch 109/200
1073/1072 [==============================] - 517s 482ms/step - loss: 0.1309 - acc: 0.9783 - val_loss: 0.2534 - val_acc: 0.9638
Epoch 110/200
1073/1072 [==============================] - 524s 488ms/step - loss: 0.1297 - acc: 0.9787 - val_loss: 0.2516 - val_acc: 0.9638
Epoch 111/200
1073/1072 [==============================] - 530s 494ms/step - loss: 0.1294 - acc: 0.9785 - val_loss: 0.2527 - val_acc: 0.9634
Epoch 112/200
1073/1072 [==============================] - 513s 478ms/step - loss: 0.1283 - acc: 0.9782 - val_loss: 0.2510 - val_acc: 0.9629
Epoch 113/200
1073/1072 [==============================] - 502s 467ms/step - loss: 0.1275 - acc: 0.9783 - val_loss: 0.2528 - val_acc: 0.9633
Epoch 114/200
1073/1072 [==============================] - 523s 487ms/step - loss: 0.1262 - acc: 0.9788 - val_loss: 0.2525 - val_acc: 0.9625
Epoch 115/200
1073/1072 [==============================] - 513s 478ms/step - loss: 0.1252 - acc: 0.9788 - val_loss: 0.2453 - val_acc: 0.9641
Epoch 116/200
1073/1072 [==============================] - 524s 488ms/step - loss: 0.1240 - acc: 0.9790 - val_loss: 0.2537 - val_acc: 0.9625
Epoch 117/200
1073/1072 [==============================] - 523s 487ms/step - loss: 0.1240 - acc: 0.9787 - val_loss: 0.2522 - val_acc: 0.9629
Epoch 118/200
1073/1072 [==============================] - 522s 487ms/step - loss: 0.1232 - acc: 0.9787 - val_loss: 0.2512 - val_acc: 0.9625
Epoch 119/200
1073/1072 [==============================] - 513s 478ms/step - loss: 0.1218 - acc: 0.9788 - val_loss: 0.2494 - val_acc: 0.9630
Epoch 120/200
1073/1072 [==============================] - 506s 472ms/step - loss: 0.1209 - acc: 0.9791 - val_loss: 0.2465 - val_acc: 0.9636
Epoch 121/200
1073/1072 [==============================] - 527s 491ms/step - loss: 0.1203 - acc: 0.9789 - val_loss: 0.2506 - val_acc: 0.9626
Epoch 122/200
1073/1072 [==============================] - 573s 534ms/step - loss: 0.1141 - acc: 0.9809 - val_loss: 0.2385 - val_acc: 0.9654
Epoch 123/200
1073/1072 [==============================] - 581s 541ms/step - loss: 0.1108 - acc: 0.9819 - val_loss: 0.2383 - val_acc: 0.9651
Epoch 124/200
1073/1072 [==============================] - 574s 535ms/step - loss: 0.1094 - acc: 0.9824 - val_loss: 0.2380 - val_acc: 0.9653
Epoch 125/200
1073/1072 [==============================] - 560s 522ms/step - loss: 0.1087 - acc: 0.9825 - val_loss: 0.2382 - val_acc: 0.9652
Epoch 126/200
1073/1072 [==============================] - 572s 533ms/step - loss: 0.1079 - acc: 0.9827 - val_loss: 0.2372 - val_acc: 0.9658
Epoch 127/200
1073/1072 [==============================] - 574s 535ms/step - loss: 0.1079 - acc: 0.9829 - val_loss: 0.2370 - val_acc: 0.9655
Epoch 128/200
1073/1072 [==============================] - 577s 537ms/step - loss: 0.1073 - acc: 0.9826 - val_loss: 0.2367 - val_acc: 0.9655
Epoch 129/200
1073/1072 [==============================] - 578s 539ms/step - loss: 0.1062 - acc: 0.9832 - val_loss: 0.2361 - val_acc: 0.9658
Epoch 130/200
1073/1072 [==============================] - 558s 520ms/step - loss: 0.1063 - acc: 0.9831 - val_loss: 0.2366 - val_acc: 0.9656
Epoch 131/200
1073/1072 [==============================] - 569s 530ms/step - loss: 0.1048 - acc: 0.9833 - val_loss: 0.2367 - val_acc: 0.9656
Epoch 132/200
1073/1072 [==============================] - 579s 539ms/step - loss: 0.1057 - acc: 0.9830 - val_loss: 0.2358 - val_acc: 0.9659
Epoch 133/200
1073/1072 [==============================] - 555s 517ms/step - loss: 0.1054 - acc: 0.9833 - val_loss: 0.2359 - val_acc: 0.9659
Epoch 134/200
1073/1072 [==============================] - 549s 512ms/step - loss: 0.1043 - acc: 0.9835 - val_loss: 0.2355 - val_acc: 0.9655
Epoch 135/200
1073/1072 [==============================] - 548s 511ms/step - loss: 0.1033 - acc: 0.9837 - val_loss: 0.2360 - val_acc: 0.9658
Epoch 136/200
1073/1072 [==============================] - 561s 523ms/step - loss: 0.1041 - acc: 0.9836 - val_loss: 0.2364 - val_acc: 0.9657
Epoch 137/200
1073/1072 [==============================] - 565s 527ms/step - loss: 0.1037 - acc: 0.9835 - val_loss: 0.2353 - val_acc: 0.9659
Epoch 138/200
1073/1072 [==============================] - 570s 531ms/step - loss: 0.1036 - acc: 0.9837 - val_loss: 0.2352 - val_acc: 0.9660
Epoch 139/200
1073/1072 [==============================] - 573s 534ms/step - loss: 0.1024 - acc: 0.9840 - val_loss: 0.2351 - val_acc: 0.9656
Epoch 140/200
1073/1072 [==============================] - 556s 518ms/step - loss: 0.1028 - acc: 0.9837 - val_loss: 0.2355 - val_acc: 0.9658
Epoch 141/200
1073/1072 [==============================] - 542s 505ms/step - loss: 0.1027 - acc: 0.9837 - val_loss: 0.2349 - val_acc: 0.9658
Epoch 142/200
1073/1072 [==============================] - 574s 535ms/step - loss: 0.1025 - acc: 0.9837 - val_loss: 0.2356 - val_acc: 0.9659
Epoch 143/200
1073/1072 [==============================] - 558s 520ms/step - loss: 0.1024 - acc: 0.9838 - val_loss: 0.2351 - val_acc: 0.9662
Epoch 144/200
1073/1072 [==============================] - 567s 528ms/step - loss: 0.1015 - acc: 0.9840 - val_loss: 0.2355 - val_acc: 0.9657
Epoch 145/200
1073/1072 [==============================] - 543s 506ms/step - loss: 0.1019 - acc: 0.9839 - val_loss: 0.2344 - val_acc: 0.9660
Epoch 146/200
1073/1072 [==============================] - 565s 527ms/step - loss: 0.1012 - acc: 0.9839 - val_loss: 0.2352 - val_acc: 0.9660
Epoch 147/200
1073/1072 [==============================] - 566s 528ms/step - loss: 0.1013 - acc: 0.9840 - val_loss: 0.2348 - val_acc: 0.9661
Epoch 148/200
1073/1072 [==============================] - 587s 547ms/step - loss: 0.1009 - acc: 0.9841 - val_loss: 0.2347 - val_acc: 0.9658
Epoch 149/200
1073/1072 [==============================] - 580s 541ms/step - loss: 0.1004 - acc: 0.9842 - val_loss: 0.2351 - val_acc: 0.9658
Epoch 150/200
1073/1072 [==============================] - 589s 549ms/step - loss: 0.1005 - acc: 0.9841 - val_loss: 0.2349 - val_acc: 0.9658
Epoch 151/200
1073/1072 [==============================] - 581s 542ms/step - loss: 0.1005 - acc: 0.9840 - val_loss: 0.2341 - val_acc: 0.9660
Epoch 152/200
1073/1072 [==============================] - 580s 541ms/step - loss: 0.1002 - acc: 0.9843 - val_loss: 0.2339 - val_acc: 0.9662
Epoch 153/200
1073/1072 [==============================] - 584s 544ms/step - loss: 0.0999 - acc: 0.9842 - val_loss: 0.2348 - val_acc: 0.9658
Epoch 154/200
1073/1072 [==============================] - 570s 531ms/step - loss: 0.1000 - acc: 0.9842 - val_loss: 0.2341 - val_acc: 0.9658
Epoch 155/200
1073/1072 [==============================] - 565s 527ms/step - loss: 0.0996 - acc: 0.9843 - val_loss: 0.2342 - val_acc: 0.9659
Epoch 156/200
1073/1072 [==============================] - 584s 544ms/step - loss: 0.0993 - acc: 0.9843 - val_loss: 0.2344 - val_acc: 0.9657
Epoch 157/200
1073/1072 [==============================] - 556s 518ms/step - loss: 0.0990 - acc: 0.9843 - val_loss: 0.2340 - val_acc: 0.9658
Epoch 158/200
1073/1072 [==============================] - 566s 528ms/step - loss: 0.0991 - acc: 0.9843 - val_loss: 0.2345 - val_acc: 0.9658
Epoch 159/200
1073/1072 [==============================] - 568s 530ms/step - loss: 0.0990 - acc: 0.9843 - val_loss: 0.2339 - val_acc: 0.9658
Epoch 160/200
1073/1072 [==============================] - 548s 511ms/step - loss: 0.0982 - acc: 0.9844 - val_loss: 0.2333 - val_acc: 0.9661
Epoch 161/200
1073/1072 [==============================] - 573s 534ms/step - loss: 0.0982 - acc: 0.9844 - val_loss: 0.2334 - val_acc: 0.9658
Epoch 162/200
1073/1072 [==============================] - 567s 529ms/step - loss: 0.0978 - acc: 0.9845 - val_loss: 0.2333 - val_acc: 0.9659
Epoch 163/200
1073/1072 [==============================] - 565s 526ms/step - loss: 0.0972 - acc: 0.9849 - val_loss: 0.2332 - val_acc: 0.9661
Epoch 164/200
1073/1072 [==============================] - 557s 519ms/step - loss: 0.0970 - acc: 0.9848 - val_loss: 0.2332 - val_acc: 0.9662
Epoch 165/200
1073/1072 [==============================] - 565s 527ms/step - loss: 0.0971 - acc: 0.9849 - val_loss: 0.2334 - val_acc: 0.9660
Epoch 166/200
1073/1072 [==============================] - 552s 514ms/step - loss: 0.0973 - acc: 0.9849 - val_loss: 0.2332 - val_acc: 0.9660
Epoch 167/200
1073/1072 [==============================] - 572s 533ms/step - loss: 0.0970 - acc: 0.9848 - val_loss: 0.2333 - val_acc: 0.9661
Epoch 168/200
1073/1072 [==============================] - 589s 549ms/step - loss: 0.0971 - acc: 0.9848 - val_loss: 0.2332 - val_acc: 0.9659
Epoch 169/200
1073/1072 [==============================] - 578s 539ms/step - loss: 0.0972 - acc: 0.9848 - val_loss: 0.2331 - val_acc: 0.9659
Epoch 170/200
1073/1072 [==============================] - 560s 522ms/step - loss: 0.0967 - acc: 0.9849 - val_loss: 0.2332 - val_acc: 0.9660
Epoch 171/200
1073/1072 [==============================] - 554s 516ms/step - loss: 0.0973 - acc: 0.9848 - val_loss: 0.2332 - val_acc: 0.9659
Epoch 172/200
1073/1072 [==============================] - 562s 524ms/step - loss: 0.0969 - acc: 0.9848 - val_loss: 0.2331 - val_acc: 0.9660
Epoch 173/200
1073/1072 [==============================] - 576s 537ms/step - loss: 0.0977 - acc: 0.9847 - val_loss: 0.2330 - val_acc: 0.9659
Epoch 174/200
1073/1072 [==============================] - 584s 544ms/step - loss: 0.0970 - acc: 0.9848 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 175/200
1073/1072 [==============================] - 563s 525ms/step - loss: 0.0970 - acc: 0.9850 - val_loss: 0.2330 - val_acc: 0.9660
Epoch 176/200
1073/1072 [==============================] - 561s 523ms/step - loss: 0.0973 - acc: 0.9847 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 177/200
1073/1072 [==============================] - 565s 526ms/step - loss: 0.0968 - acc: 0.9849 - val_loss: 0.2329 - val_acc: 0.9660
Epoch 178/200
1073/1072 [==============================] - 569s 531ms/step - loss: 0.0968 - acc: 0.9847 - val_loss: 0.2325 - val_acc: 0.9660
Epoch 179/200
1073/1072 [==============================] - 554s 516ms/step - loss: 0.0967 - acc: 0.9849 - val_loss: 0.2327 - val_acc: 0.9660
Epoch 180/200
1073/1072 [==============================] - 576s 537ms/step - loss: 0.0967 - acc: 0.9850 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 181/200
1073/1072 [==============================] - 568s 529ms/step - loss: 0.0966 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 182/200
1073/1072 [==============================] - 547s 510ms/step - loss: 0.0961 - acc: 0.9850 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 183/200
1073/1072 [==============================] - 573s 534ms/step - loss: 0.0965 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 184/200
1073/1072 [==============================] - 557s 519ms/step - loss: 0.0967 - acc: 0.9847 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 185/200
1073/1072 [==============================] - 554s 516ms/step - loss: 0.0964 - acc: 0.9849 - val_loss: 0.2329 - val_acc: 0.9659
Epoch 186/200
1073/1072 [==============================] - 555s 517ms/step - loss: 0.0964 - acc: 0.9850 - val_loss: 0.2330 - val_acc: 0.9660
Epoch 187/200
1073/1072 [==============================] - 580s 540ms/step - loss: 0.0964 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 188/200
1073/1072 [==============================] - 533s 497ms/step - loss: 0.0967 - acc: 0.9849 - val_loss: 0.2330 - val_acc: 0.9659
Epoch 189/200
1073/1072 [==============================] - 554s 517ms/step - loss: 0.0968 - acc: 0.9850 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 190/200
1073/1072 [==============================] - 550s 513ms/step - loss: 0.0965 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9660
Epoch 191/200
1073/1072 [==============================] - 528s 492ms/step - loss: 0.0962 - acc: 0.9852 - val_loss: 0.2328 - val_acc: 0.9658
Epoch 192/200
1073/1072 [==============================] - 551s 513ms/step - loss: 0.0966 - acc: 0.9848 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 193/200
1073/1072 [==============================] - 543s 506ms/step - loss: 0.0967 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9658
Epoch 194/200
1073/1072 [==============================] - 549s 511ms/step - loss: 0.0961 - acc: 0.9850 - val_loss: 0.2327 - val_acc: 0.9659
Epoch 195/200
1073/1072 [==============================] - 557s 519ms/step - loss: 0.0965 - acc: 0.9848 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 196/200
1073/1072 [==============================] - 549s 512ms/step - loss: 0.0966 - acc: 0.9850 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 197/200
1073/1072 [==============================] - 526s 490ms/step - loss: 0.0960 - acc: 0.9852 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 198/200
1073/1072 [==============================] - 540s 504ms/step - loss: 0.0965 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9659
Epoch 199/200
1073/1072 [==============================] - 542s 505ms/step - loss: 0.0962 - acc: 0.9849 - val_loss: 0.2327 - val_acc: 0.9660
Epoch 200/200
1073/1072 [==============================] - 540s 503ms/step - loss: 0.0965 - acc: 0.9849 - val_loss: 0.2328 - val_acc: 0.9660
Saved trained model at .\result_recog\test191002_aspect_ver9sv2\my_resnet\trained_model.h5
Traceback (most recent call last):
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\kuzushiji_project2.py", line 98, in <module>
    pipe_line.ResNetPipeline_191002AspectVer9sv2().run_train()
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\pipe_line.py", line 7660, in run_train
    pred_tr_letter_nos = self.__predict_using_input(tr_inputs)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\pipe_line.py", line 7701, in __predict_using_input
    soft=soft)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\char_classification\resnet.py", line 1748, in predict_tta
    auged_imgs_set = tta_func(images, other_inputs)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\char_classification\tta.py", line 42, in augment_image
    auged_imgs = self.__translate(images, w_sft, h_sft)
  File "C:\Users\stnu2\Documents\Kaggle\kuzushiji-recognition\kuzushiji_project2\kuzushiji_project2\src\char_classification\tta.py", line 63, in __translate
    translated_images = np.pad(images, pad_width=(pad_w_0, pad_w_1, pad_w_2, pad_w_3), mode='edge')
  File "C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\numpy\lib\arraypad.py", line 1252, in pad
    newmat = _append_edge(newmat, pad_after, axis)
  File "C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\numpy\lib\arraypad.py", line 218, in _append_edge
    return _do_append(arr, edge_arr.repeat(pad_amt, axis=axis), axis)
  File "C:\Users\stnu2\Anaconda3\envs\kuzushiji\lib\site-packages\numpy\lib\arraypad.py", line 104, in _do_append
    (arr, pad_chunk.astype(arr.dtype, copy=False)), axis=axis)
MemoryError
続行するには何かキーを押してください . . .